name: Deploy Data Pipeline Infrastructure

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'glue_jobs/**'
      - 'snowflake/**'
      - 'scripts/**'
      - '.github/workflows/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'glue_jobs/**'
      - 'snowflake/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment for SQL generation'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod
      skip_deployment:
        description: 'Skip AWS/Snowflake deployment (SQL generation only)'
        required: false
        default: false
        type: boolean
      commit_generated_files:
        description: 'Commit generated SQL files to repository'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  GLUE_ROLE_NAME: data-engineer-user
  GLUE_DATABASE_NAME: data_pipeline_db
  S3_SCRIPTS_PREFIX: glue-scripts/
  # SQL Generation environment variables (EXAMPLE)
  SNOWFLAKE_DATABASE_DEV: "ECOMMERCE_DB"
  SNOWFLAKE_DATABASE_STAGING: "ECOMMERCE_STAGING_DB" 
  SNOWFLAKE_DATABASE_PROD: "ECOMMERCE_PROD_DB"
  SNOWFLAKE_WAREHOUSE_DEV: "INT_WH"
  SNOWFLAKE_WAREHOUSE_STAGING: "COMPUTE_WH_STAGING"
  SNOWFLAKE_WAREHOUSE_PROD: "COMPUTE_WH_PROD"
  S3_BUCKET_DEV: "my-amazing-app"
  S3_BUCKET_STAGING: "my-amazing-app-staging"
  S3_BUCKET_PROD: "my-amazing-app-prod"

jobs:
  deploy-snowflake:
    name: Deploy Snowflake Infrastructure
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event.inputs.skip_deployment != 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install snowflake-connector-python==3.7.0 python-dotenv
        
    - name: Deploy Snowflake tables
      env:
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        echo "Setting up Snowflake external tables..."
        python snowflake_connector.py
        
    - name: Snowflake deployment summary
      run: |
        echo "Snowflake infrastructure deployed successfully!"
        echo "External tables created for S3 data"
        echo "Ready for analytics queries"

  deploy-aws-glue:
    name: Deploy AWS Glue Jobs
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event.inputs.skip_deployment != 'true'
    needs: deploy-snowflake
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install AWS CLI and dependencies
      run: |
        pip install boto3 awscli
        
    - name: Create Glue database and catalog tables
      run: |
        echo "Creating Glue catalog database and tables..."
        
        # Install required Python packages
        pip install python-dotenv
        
        # Create database and all catalog tables
        python scripts/setup_all_catalog_tables.py
        
        echo "✅ All catalog tables created successfully!"
        
    - name: Create/Update IAM Role for Glue
      run: |
        echo "Setting up IAM role for Glue jobs..."
        
        # Create trust policy
        cat > trust-policy.json << EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Service": "glue.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
        EOF
        
        # Create role (ignore error if exists)
        aws iam create-role \
          --role-name ${{ env.GLUE_ROLE_NAME }} \
          --assume-role-policy-document file://trust-policy.json \
          --description "IAM role for Data Pipeline Glue jobs" || echo "Role already exists"
        
        # Attach managed policies
        aws iam attach-role-policy \
          --role-name ${{ env.GLUE_ROLE_NAME }} \
          --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole || true
        
        aws iam attach-role-policy \
          --role-name ${{ env.GLUE_ROLE_NAME }} \
          --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess || true
        
        aws iam attach-role-policy \
          --role-name ${{ env.GLUE_ROLE_NAME }} \
          --policy-arn arn:aws:iam::aws:policy/CloudWatchLogsFullAccess || true
        
        # Create custom policy for Iceberg
        cat > iceberg-policy.json << EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": [
                "glue:*",
                "lakeformation:GetDataAccess",
                "lakeformation:GrantPermissions",
                "lakeformation:BatchGrantPermissions",
                "lakeformation:RevokePermissions",
                "lakeformation:BatchRevokePermissions",
                "lakeformation:ListPermissions"
              ],
              "Resource": "*"
            }
          ]
        }
        EOF
        
        aws iam put-role-policy \
          --role-name ${{ env.GLUE_ROLE_NAME }} \
          --policy-name IcebergCustomPolicy \
          --policy-document file://iceberg-policy.json || true
        
    - name: Upload Glue job scripts to S3
      run: |
        echo "Uploading Glue job scripts to S3..."
        
        # Upload all Glue job scripts
        aws s3 cp glue_jobs/01_raw_data_transformation.py \
          s3://${{ secrets.S3_BUCKET_NAME }}/${{ env.S3_SCRIPTS_PREFIX }}01_raw_data_transformation.py
        
        aws s3 cp glue_jobs/02_analytics_aggregation.py \
          s3://${{ secrets.S3_BUCKET_NAME }}/${{ env.S3_SCRIPTS_PREFIX }}02_analytics_aggregation.py
        
        aws s3 cp glue_jobs/03_time_series_analysis.py \
          s3://${{ secrets.S3_BUCKET_NAME }}/${{ env.S3_SCRIPTS_PREFIX }}03_time_series_analysis.py
        
        echo "✅ All scripts uploaded successfully!"
        
    - name: Create/Update Glue Jobs
      run: |
        echo "Creating/updating Glue jobs..."
        
        # Get the role ARN
        ROLE_ARN=$(aws iam get-role --role-name ${{ env.GLUE_ROLE_NAME }} --query 'Role.Arn' --output text)
        echo "Using role: $ROLE_ARN"

        aws glue delete-job --job-name data-pipeline-raw-transformation || true
        aws glue delete-job --job-name data-pipeline-analytics-aggregation || true
        aws glue delete-job --job-name data-pipeline-time-series-analysis || true

        
        # Job 1: Raw Data Transformation
        cat > job1-config.json << EOF
        {
          "Name": "data-pipeline-raw-transformation",
          "Description": "Transform raw user data with validation and enrichment",
          "Role": "$ROLE_ARN",
          "Command": {
            "Name": "glueetl",
            "ScriptLocation": "s3://${{ secrets.S3_BUCKET_NAME }}/${{ env.S3_SCRIPTS_PREFIX }}01_raw_data_transformation.py",
            "PythonVersion": "3"
          },
          "DefaultArguments": {
            "--job-language": "python",
            "--enable-metrics": "",
            "--enable-continuous-cloudwatch-log": "true",
            "--enable-spark-ui": "true",
            "--spark-event-logs-path": "s3://${{ secrets.S3_BUCKET_NAME }}/spark-logs/"
          },
          "MaxRetries": 1,
          "Timeout": 60,
          "MaxCapacity": 2.0,
          "GlueVersion": "4.0"
        }
        EOF
        
        aws glue create-job --cli-input-json file://job1-config.json
        
        # Job 2: Analytics Aggregation
        cat > job2-config.json << EOF
        {
          "Name": "data-pipeline-analytics-aggregation",
          "Description": "Create analytics aggregations and dimensional models",
          "Role": "$ROLE_ARN",
          "Command": {
            "Name": "glueetl",
            "ScriptLocation": "s3://${{ secrets.S3_BUCKET_NAME }}/${{ env.S3_SCRIPTS_PREFIX }}02_analytics_aggregation.py",
            "PythonVersion": "3"
          },
          "DefaultArguments": {
            "--job-language": "python",
            "--enable-metrics": "",
            "--enable-continuous-cloudwatch-log": "true",
            "--enable-spark-ui": "true",
            "--spark-event-logs-path": "s3://${{ secrets.S3_BUCKET_NAME }}/spark-logs/"
          },
          "MaxRetries": 1,
          "Timeout": 60,
          "MaxCapacity": 2.0,
          "GlueVersion": "4.0"
        }
        EOF
        
        aws glue create-job --cli-input-json file://job2-config.json
        
        # Job 3: Time Series Analysis
        cat > job3-config.json << EOF
        {
          "Name": "data-pipeline-time-series-analysis",
          "Description": "Time series analysis and trend detection",
          "Role": "$ROLE_ARN",
          "Command": {
            "Name": "glueetl",
            "ScriptLocation": "s3://${{ secrets.S3_BUCKET_NAME }}/${{ env.S3_SCRIPTS_PREFIX }}03_time_series_analysis.py",
            "PythonVersion": "3"
          },
          "DefaultArguments": {
            "--job-language": "python",
            "--enable-metrics": "",
            "--enable-continuous-cloudwatch-log": "true",
            "--enable-spark-ui": "true",
            "--spark-event-logs-path": "s3://${{ secrets.S3_BUCKET_NAME }}/spark-logs/"
          },
          "MaxRetries": 1,
          "Timeout": 60,
          "MaxCapacity": 2.0,
          "GlueVersion": "4.0"
        }
        EOF
        
        aws glue create-job --cli-input-json file://job3-config.json
        
        echo "✅ All Glue jobs created successfully!"
        
    - name: ✅ AWS deployment summary
      run: |
        echo "AWS Glue infrastructure deployed successfully!"
        echo "3 Glue jobs created and ready to run"
        echo "Data catalog tables configured"
        echo "IAM roles and policies configured"

  generate-sql-files:
    name: Generate Snowflake SQL Files
    runs-on: ubuntu-latest
    needs: [deploy-snowflake, deploy-aws-glue]
    if: always() && (needs.deploy-snowflake.result == 'success' || needs.deploy-snowflake.result == 'skipped') && (needs.deploy-aws-glue.result == 'success' || needs.deploy-aws-glue.result == 'skipped')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Determine target environment
      id: env
      run: |
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          ENV="${{ github.event.inputs.environment }}"
        elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          ENV="dev"
        elif [[ "${{ github.ref }}" == "refs/heads/prod" ]]; then
          ENV="prod"
        else
          ENV="dev"
        fi
        echo "environment=${ENV}" >> $GITHUB_OUTPUT
        echo "Target environment: ${ENV}"
        
    - name: Generate SQL files for target environment  
      run: |
        python scripts/generate_snowflake_sql.py ${{ steps.env.outputs.environment }}
        
    - name: Create deployment summary
      run: |
        ENV="${{ steps.env.outputs.environment }}"
        echo "## SQL Generation Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** \`${ENV}\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -d "generated_sql/${ENV}" ]; then
          for file in generated_sql/${ENV}/*.sql; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo "Unknown")
              echo "- ✅ \`${filename}\` (${size} bytes)" >> $GITHUB_STEP_SUMMARY
            fi
          done
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Environment Configuration:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        case ${ENV} in
          "dev")
            echo "- **Database:** \`${{ env.SNOWFLAKE_DATABASE_DEV }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Warehouse:** \`${{ env.SNOWFLAKE_WAREHOUSE_DEV }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **S3 Bucket:** \`${{ env.S3_BUCKET_DEV }}\`" >> $GITHUB_STEP_SUMMARY
            ;;
          "staging")
            echo "- **Database:** \`${{ env.SNOWFLAKE_DATABASE_STAGING }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Warehouse:** \`${{ env.SNOWFLAKE_WAREHOUSE_STAGING }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **S3 Bucket:** \`${{ env.S3_BUCKET_STAGING }}\`" >> $GITHUB_STEP_SUMMARY
            ;;
          "prod")
            echo "- **Database:** \`${{ env.SNOWFLAKE_DATABASE_PROD }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Warehouse:** \`${{ env.SNOWFLAKE_WAREHOUSE_PROD }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **S3 Bucket:** \`${{ env.S3_BUCKET_PROD }}\`" >> $GITHUB_STEP_SUMMARY
            ;;
        esac
        
    - name: Upload generated SQL files as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: snowflake-sql-${{ steps.env.outputs.environment }}
        path: generated_sql/${{ steps.env.outputs.environment }}/
        retention-days: 30
        
    - name: List generated files
      run: |
        echo "Generated SQL files for ${{ steps.env.outputs.environment }} environment:"
        ls -la generated_sql/${{ steps.env.outputs.environment }}/
        echo ""
        echo "File sizes:"
        du -h generated_sql/${{ steps.env.outputs.environment }}/*
        
    - name: Commit generated files to repository
      run: |
        ENV="${{ steps.env.outputs.environment }}"
        echo "Committing generated SQL files for ${ENV} environment..."
        
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add generated files
        git add generated_sql/${ENV}/
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "ℹ️  No changes to commit"
        else
          # Commit changes
          git commit -m "🔧 Auto-generate SQL files for ${ENV} environment
          
          Generated on: $(date)
          Workflow: ${{ github.workflow }}
          Run ID: ${{ github.run_id }}
          Commit: ${{ github.sha }}
          
          Files generated:
          $(find generated_sql/${ENV} -name "*.sql" -exec basename {} \; | sort)"
          
          # Push changes
          git push
          echo "✅ Generated SQL files committed and pushed successfully!"
        fi
        
  validate-generated-sql:
    name: ✅ Validate Generated SQL
    runs-on: ubuntu-latest
    needs: generate-sql-files
    if: always() && needs.generate-sql-files.result == 'success'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Determine target environment
      id: env
      run: |
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          ENV="${{ github.event.inputs.environment }}"
        elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          ENV="dev"
        elif [[ "${{ github.ref }}" == "refs/heads/prod" ]]; then
          ENV="prod"
        else
          ENV="dev"
        fi
        echo "environment=${ENV}" >> $GITHUB_OUTPUT
        
    - name: Download generated SQL files
      uses: actions/download-artifact@v4
      with:
        name: snowflake-sql-${{ steps.env.outputs.environment }}
        path: generated_sql/${{ steps.env.outputs.environment }}/
        
    - name: Validate SQL syntax and content
      run: |
        ENV="${{ steps.env.outputs.environment }}"
        echo "Validating generated SQL files for ${ENV} environment..."
        
        # Check if files were generated
        if [ ! -d "generated_sql/${ENV}" ]; then
          echo "❌ Generated SQL directory not found!"
          exit 1
        fi
        
        # Count generated files
        sql_count=$(find generated_sql/${ENV} -name "*.sql" | wc -l)
        echo "Found ${sql_count} SQL files"
        
        if [ ${sql_count} -eq 0 ]; then
          echo "❌ No SQL files were generated!"
          exit 1
        fi
        
        # Validate each SQL file
        for sql_file in generated_sql/${ENV}/*.sql; do
          if [ -f "$sql_file" ]; then
            filename=$(basename "$sql_file")
            echo "Validating ${filename}..."
            
            # Check file is not empty
            if [ ! -s "$sql_file" ]; then
              echo "❌ ${filename} is empty!"
              exit 1
            fi
            
            # Check for template variables that weren't replaced
            if grep -q '\${' "$sql_file"; then
              echo "⚠️  ${filename} contains unreplaced variables:"
              grep '\${' "$sql_file" || true
            fi
            
            # Basic SQL syntax check (look for common keywords)
            if ! grep -qi -E "(CREATE|SELECT|INSERT|UPDATE|DELETE|DROP|ALTER)" "$sql_file"; then
              echo "⚠️  ${filename} might not contain valid SQL statements"
            fi
            
            # Check for environment-specific values
            case ${ENV} in
              "dev")
                if ! grep -q "DEV" "$sql_file"; then
                  echo "⚠️  ${filename} might not contain DEV environment values"
                fi
                ;;
              "staging")
                if ! grep -q "STAGING" "$sql_file"; then
                  echo "⚠️  ${filename} might not contain STAGING environment values"
                fi
                ;;
              "prod")
                if ! grep -q "PROD" "$sql_file"; then
                  echo "⚠️  ${filename} might not contain PROD environment values"
                fi
                ;;
            esac
            
            echo "✅ ${filename} validation completed"
          fi
        done
        
        echo "All SQL files validated successfully!"
        
    - name: Generate validation report
      run: |
        ENV="${{ steps.env.outputs.environment }}"
        echo "## ✅ SQL Validation Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** \`${ENV}\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # File validation summary
        sql_count=$(find generated_sql/${ENV} -name "*.sql" | wc -l)
        echo "### Validation Summary:" >> $GITHUB_STEP_SUMMARY
        echo "- Files Generated:** ${sql_count}" >> $GITHUB_STEP_SUMMARY
        echo "- Validation Status:** Passed" >> $GITHUB_STEP_SUMMARY
        echo "- Environment:** ${ENV}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "### File Details:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| File | Size | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|------|------|--------|" >> $GITHUB_STEP_SUMMARY
        
        for sql_file in generated_sql/${ENV}/*.sql; do
          if [ -f "$sql_file" ]; then
            filename=$(basename "$sql_file")
            size=$(stat -f%z "$sql_file" 2>/dev/null || stat -c%s "$sql_file" 2>/dev/null || echo "Unknown")
            echo "| \`${filename}\` | ${size} bytes | ✅ Valid |" >> $GITHUB_STEP_SUMMARY
          fi
        done

  deployment-summary:
    name: Deployment Summary
    runs-on: ubuntu-latest
    needs: [deploy-snowflake, deploy-aws-glue, generate-sql-files, validate-generated-sql]
    if: always()
    
    steps:
    - name: Create final deployment summary
      run: |
        echo "## Pipeline Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Job Status:" >> $GITHUB_STEP_SUMMARY
        echo "- **Snowflake Deployment:** ${{ needs.deploy-snowflake.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **AWS Glue Deployment:** ${{ needs.deploy-aws-glue.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **SQL Generation:** ${{ needs.generate-sql-files.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **SQL Validation:** ${{ needs.validate-generated-sql.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Determine overall status
        if [[ "${{ needs.deploy-snowflake.result }}" == "success" && "${{ needs.deploy-aws-glue.result }}" == "success" && "${{ needs.generate-sql-files.result }}" == "success" && "${{ needs.validate-generated-sql.result }}" == "success" ]]; then
          echo "### Overall Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
          echo "All pipeline components deployed successfully!" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.generate-sql-files.result }}" == "success" && "${{ needs.validate-generated-sql.result }}" == "success" ]]; then
          echo "### Overall Status: SQL GENERATION SUCCESS" >> $GITHUB_STEP_SUMMARY
          echo "SQL files generated and validated successfully!" >> $GITHUB_STEP_SUMMARY
        else
          echo "### ⚠️  Overall Status: PARTIAL SUCCESS" >> $GITHUB_STEP_SUMMARY
          echo "Some components may have failed. Check individual job logs." >> $GITHUB_STEP_SUMMARY
        fi 
